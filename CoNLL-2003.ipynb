{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (\n",
    "    LSTM, \n",
    "    Embedding,\n",
    "    Dense,\n",
    "    TimeDistributed,\n",
    "    Dropout,\n",
    "    Bidirectional,\n",
    "    Activation\n",
    ")\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the data processing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_labels_one(filename,encoding='utf-8'):\n",
    "    \"\"\"This function is used for loading the data and converting the same \n",
    "    into a structured datastucture, other than splitting here I am removing \n",
    "    the special charecters specially.\n",
    "    \"\"\"\n",
    "    words=[]\n",
    "    pos=[]    \n",
    "    sent=[]\n",
    "    label1=[]\n",
    "    tempz=\"\"\"(),.<>?$#@\"!%&*:;'~`^=-_+\\|{}[]/\"\"\"\n",
    "    removal=[i for i in tempz]\n",
    "    with open(filename, encoding=encoding) as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            if line:\n",
    "                word, pos_,parser_,ner_ = line.split()\n",
    "                if word!=\"-DOCSTART-\" and word not in removal and pos_ not in removal:\n",
    "                    words.append(word)\n",
    "                    pos.append(pos_)\n",
    "            else:\n",
    "                if words!=[] and pos!=[]:\n",
    "                    sent.append(words)\n",
    "                    label1.append(pos)\n",
    "                    words,pos= [], []\n",
    "\n",
    "    return sent, label1\n",
    "\n",
    "def load_data_and_labels_two(filename,encoding='utf-8'):\n",
    "    \"\"\"\n",
    "    This function also works same like the previous one but here \n",
    "    I am not removing the special charecters.\n",
    "    \"\"\"\n",
    "    words=[]\n",
    "    parser=[]\n",
    "    ner=[]    \n",
    "    sent=[]\n",
    "    label2=[]\n",
    "    label3=[]\n",
    "    tempz=\"\"\"(),.<>?$#@\"!%&*:;'~`^=-_+\\|{}[]/\"\"\"\n",
    "    removal=[i for i in tempz]\n",
    "    with open(filename, encoding=encoding) as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            if line:\n",
    "                word, pos_,parser_,ner_ = line.split()\n",
    "                if word!=\"-DOCSTART-\" and parser_ not in removal and ner_ not in removal:\n",
    "                    words.append(word)\n",
    "                    parser.append(parser_)\n",
    "                    ner.append(ner_)\n",
    "            else:\n",
    "                if words!=[] and ner!=[] and parser!=[]:\n",
    "                    sent.append(words)\n",
    "                    label2.append(parser)\n",
    "                    label3.append(ner)\n",
    "                    words, parser, ner = [], [], []\n",
    "\n",
    "    return sent, label2, label3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_uniques(arr_x, arr_y):\n",
    "    \n",
    "    tmp_x, tmp_y = [], []\n",
    "    \n",
    "    for idx in arr_x: \n",
    "        for x in idx: \n",
    "            tmp_x.append(x)\n",
    "  \n",
    "    for idx in arr_y: \n",
    "        for x in idx:\n",
    "            tmp_y.append(x)\n",
    "    return list(set(tmp_x)), list(set(tmp_y))\n",
    "\n",
    "## Function for word to index.\n",
    "def word2idx(all_words): \n",
    "\n",
    "    tmp = {value: idx + 2 for idx, value in enumerate(all_words)}\n",
    "    tmp[\"UNK\"] = 1 \n",
    "    tmp[\"PAD\"] = 0\n",
    "    \n",
    "    return tmp\n",
    "# Function for tag to index.\n",
    "def tag2idx(all_tags):\n",
    "    \n",
    "    tmp = {value: idx + 1 for idx, value in enumerate(all_tags)}\n",
    "    tmp[\"PAD\"] = 0 \n",
    "    \n",
    "    return tmp\n",
    "# Function for index to word.\n",
    "def idx2word(word2idx):\n",
    "    \n",
    "    return {idx: value for value, idx in word2idx.items()}\n",
    "# Function for index to tag.\n",
    "def idx2tag(tag2idx):\n",
    "    \n",
    "    return {idx: value for value, idx in tag2idx.items()}\n",
    "\n",
    "\n",
    "# Function used to convert the list of words and tags into a model friendly format.\n",
    "def parser_arrays(MAX_LEN,x_train, y_train, all_words, all_tags):\n",
    "    \n",
    "    obj_word2idx = word2idx(all_words)\n",
    "    obj_tag2idx = tag2idx(all_tags)\n",
    "    \n",
    "    __X = [[obj_word2idx[x] for x in value] for value in x_train] \n",
    "    __y = [[obj_tag2idx[x] for x in value] for value in y_train]\n",
    "\n",
    "    #Terceira Parte\n",
    "    X_pad = pad_sequences(maxlen=MAX_LEN, sequences=__X, padding=\"post\", value=0)\n",
    "    y_pad = pad_sequences(maxlen=MAX_LEN, sequences=__y, padding=\"post\", value=0)\n",
    "    \n",
    "    return  X_pad, np.array([to_categorical(idx, num_classes=len(all_tags) + 1) for idx in y_pad])\n",
    "\n",
    "# Function used during inference for returning the predicted tag with the original tag.  \n",
    "def parser2categorical(pred, y_true, all_tags):\n",
    "    \n",
    "    k = tag2idx(all_tags)\n",
    "    parser_idx = idx2tag(k)\n",
    "\n",
    "    pred_tag = [[parser_idx[idx] for idx in row] for row in pred]\n",
    "    y_true_tag = [[parser_idx[idx] for idx in row] for row in y_true] \n",
    "    \n",
    "    return pred_tag, y_true_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_model(NUM_WORDS,MAX_LEN,NUM_TAGS):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=NUM_WORDS, output_dim=MAX_LEN,input_length=MAX_LEN))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Bidirectional(LSTM(units=MAX_LEN,return_sequences=True,recurrent_dropout=0.1)))\n",
    "    model.add(TimeDistributed(Dense(units=NUM_TAGS)))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(optimizer='adam', \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS - Part of Speach - multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data and converting into model consumeable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_data_and_labels_one(\"conll-2003/eng.train\")\n",
    "X_valid, y_valid= load_data_and_labels_one(\"conll-2003/eng.testa\")\n",
    "X_teste, y_teste= load_data_and_labels_one(\"conll-2003/eng.testb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words, all_tags = build_uniques(X_train + X_valid + X_teste, y_train)\n",
    "\n",
    "MAX_LEN = max([len(x) for x in X_train + X_valid + X_teste])\n",
    "NUM_WORDS = len(all_words) + 2\n",
    "NUM_TAGS = len(all_tags) + 1\n",
    "\n",
    "X_train, y_train = parser_arrays(MAX_LEN,X_train, y_train, all_words, all_tags)\n",
    "X_valid, y_valid = parser_arrays(MAX_LEN,X_valid, y_valid, all_words, all_tags)\n",
    "X_teste, y_teste = parser_arrays(MAX_LEN,X_teste, y_teste, all_words, all_tags)\n",
    "\n",
    "with open('encoding/X_teste_POS.pkl', 'wb') as f:\n",
    "    pickle.dump(X_teste, f)\n",
    "with open('encoding/y_teste_POS.pkl', 'wb') as f:\n",
    "    pickle.dump(y_teste, f)\n",
    "with open('encoding/all_words_POS.pkl', 'wb') as f:\n",
    "    pickle.dump(all_words, f)\n",
    "with open('encoding/all_tags_POS.pkl', 'wb') as f:\n",
    "    pickle.dump(all_tags, f)\n",
    "with open('encoding/MAX_LEN_POS.pkl', 'wb') as f:\n",
    "    pickle.dump(MAX_LEN, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model and saving the weights for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14011 samples, validate on 3242 samples\n",
      "Epoch 1/5\n",
      "14011/14011 [==============================] - 49s 3ms/step - loss: 0.5856 - accuracy: 0.8674 - val_loss: 0.3976 - val_accuracy: 0.8868\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.39761, saving model to checkpoints/model_PAR.h5\n",
      "Epoch 2/5\n",
      "14011/14011 [==============================] - 55s 4ms/step - loss: 0.2975 - accuracy: 0.9205 - val_loss: 0.2464 - val_accuracy: 0.9326\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.39761 to 0.24636, saving model to checkpoints/model_PAR.h5\n",
      "Epoch 3/5\n",
      "14011/14011 [==============================] - 55s 4ms/step - loss: 0.1583 - accuracy: 0.9578 - val_loss: 0.1259 - val_accuracy: 0.9692\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.24636 to 0.12590, saving model to checkpoints/model_PAR.h5\n",
      "Epoch 4/5\n",
      "14011/14011 [==============================] - 55s 4ms/step - loss: 0.0767 - accuracy: 0.9827 - val_loss: 0.0788 - val_accuracy: 0.9808\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.12590 to 0.07881, saving model to checkpoints/model_PAR.h5\n",
      "Epoch 5/5\n",
      "14011/14011 [==============================] - 56s 4ms/step - loss: 0.0440 - accuracy: 0.9901 - val_loss: 0.0645 - val_accuracy: 0.9833\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.07881 to 0.06453, saving model to checkpoints/model_PAR.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x27e2f8255c0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=simple_model(NUM_WORDS,MAX_LEN,NUM_TAGS)\n",
    "filename = 'checkpoints/model_PAR.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=5,  \n",
    "    validation_data = [X_valid, y_valid],  \n",
    "    callbacks=[checkpoint],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntactic Chunk Tags - multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data and converting into model consumeable format and training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14041 samples, validate on 3250 samples\n",
      "Epoch 1/5\n",
      "14041/14041 [==============================] - 87s 6ms/step - loss: 0.2742 - accuracy: 0.9386 - val_loss: 0.1434 - val_accuracy: 0.9517\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14337, saving model to checkpoints/model_PAR.h5\n",
      "Epoch 2/5\n",
      "14041/14041 [==============================] - 97s 7ms/step - loss: 0.0744 - accuracy: 0.9793 - val_loss: 0.0496 - val_accuracy: 0.9870\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14337 to 0.04956, saving model to checkpoints/model_PAR.h5\n",
      "Epoch 3/5\n",
      "14041/14041 [==============================] - 101s 7ms/step - loss: 0.0358 - accuracy: 0.9905 - val_loss: 0.0369 - val_accuracy: 0.9900\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04956 to 0.03687, saving model to checkpoints/model_PAR.h5\n",
      "Epoch 4/5\n",
      "14041/14041 [==============================] - 100s 7ms/step - loss: 0.0259 - accuracy: 0.9927 - val_loss: 0.0312 - val_accuracy: 0.9913\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03687 to 0.03119, saving model to checkpoints/model_PAR.h5\n",
      "Epoch 5/5\n",
      "14041/14041 [==============================] - 102s 7ms/step - loss: 0.0203 - accuracy: 0.9942 - val_loss: 0.0286 - val_accuracy: 0.9921\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03119 to 0.02857, saving model to checkpoints/model_PAR.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x27e31974550>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1, y_train1, y_train2 = load_data_and_labels_two(\"conll-2003/eng.train\")\n",
    "X_valid1, y_valid1, y_valid2= load_data_and_labels_two(\"conll-2003/eng.testa\")\n",
    "X_teste1, y_teste1, y_teste2= load_data_and_labels_two(\"conll-2003/eng.testb\")\n",
    "\n",
    "\n",
    "all_words, all_tags = build_uniques(X_train1 + X_valid1 + X_teste1, y_train1)\n",
    "\n",
    "MAX_LEN = max([len(x) for x in X_train1 + X_valid1 + X_teste1])\n",
    "NUM_WORDS = len(all_words) + 2\n",
    "NUM_TAGS = len(all_tags) + 1\n",
    "\n",
    "X_train, y_train = parser_arrays(MAX_LEN,X_train1, y_train1, all_words, all_tags)\n",
    "X_valid, y_valid = parser_arrays(MAX_LEN,X_valid1, y_valid1, all_words, all_tags)\n",
    "X_teste, y_teste = parser_arrays(MAX_LEN,X_teste1, y_teste1, all_words, all_tags)\n",
    "\n",
    "\n",
    "with open('encoding/X_teste_PAR.pkl', 'wb') as f:\n",
    "    pickle.dump(X_teste, f)\n",
    "with open('encoding/y_teste_PAR.pkl', 'wb') as f:\n",
    "    pickle.dump(y_teste, f)\n",
    "with open('encoding/all_words_PAR.pkl', 'wb') as f:\n",
    "    pickle.dump(all_words, f)\n",
    "with open('encoding/all_tags_PAR.pkl', 'wb') as f:\n",
    "    pickle.dump(all_tags, f)\n",
    "with open('encoding/MAX_LEN_PAR.pkl', 'wb') as f:\n",
    "    pickle.dump(MAX_LEN, f)\n",
    "\n",
    "model=simple_model(NUM_WORDS,MAX_LEN,NUM_TAGS)\n",
    "filename = 'checkpoints/model_PAR.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=5,  \n",
    "    validation_data = [X_valid, y_valid],  \n",
    "    callbacks=[checkpoint],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER - multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data and converting into model consumeable format and training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14041 samples, validate on 3250 samples\n",
      "Epoch 1/5\n",
      "14041/14041 [==============================] - 104s 7ms/step - loss: 0.1702 - accuracy: 0.9638 - val_loss: 0.0741 - val_accuracy: 0.9786\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.07415, saving model to checkpoints/model_NER.h5\n",
      "Epoch 2/5\n",
      "14041/14041 [==============================] - 108s 8ms/step - loss: 0.0439 - accuracy: 0.9851 - val_loss: 0.0365 - val_accuracy: 0.9885\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.07415 to 0.03645, saving model to checkpoints/model_NER.h5\n",
      "Epoch 3/5\n",
      "14041/14041 [==============================] - 108s 8ms/step - loss: 0.0205 - accuracy: 0.9940 - val_loss: 0.0238 - val_accuracy: 0.9931\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03645 to 0.02381, saving model to checkpoints/model_NER.h5\n",
      "Epoch 4/5\n",
      "14041/14041 [==============================] - 110s 8ms/step - loss: 0.0094 - accuracy: 0.9976 - val_loss: 0.0198 - val_accuracy: 0.9944\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02381 to 0.01984, saving model to checkpoints/model_NER.h5\n",
      "Epoch 5/5\n",
      "14041/14041 [==============================] - 111s 8ms/step - loss: 0.0052 - accuracy: 0.9987 - val_loss: 0.0185 - val_accuracy: 0.9948\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01984 to 0.01845, saving model to checkpoints/model_NER.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x27e2a9024a8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1, y_train1, y_train2 = load_data_and_labels_two(\"conll-2003/eng.train\")\n",
    "X_valid1, y_valid1, y_valid2= load_data_and_labels_two(\"conll-2003/eng.testa\")\n",
    "X_teste1, y_teste1, y_teste2= load_data_and_labels_two(\"conll-2003/eng.testb\")\n",
    "\n",
    "\n",
    "all_words, all_tags = build_uniques(X_train1 + X_valid1 + X_teste1, y_train2)\n",
    "\n",
    "MAX_LEN = max([len(x) for x in X_train1 + X_valid1 + X_teste1])\n",
    "NUM_WORDS = len(all_words) + 2\n",
    "NUM_TAGS = len(all_tags) + 1\n",
    "\n",
    "X_train, y_train = parser_arrays(MAX_LEN,X_train1, y_train2, all_words, all_tags)\n",
    "X_valid, y_valid = parser_arrays(MAX_LEN,X_valid1, y_valid2, all_words, all_tags)\n",
    "X_teste, y_teste = parser_arrays(MAX_LEN,X_teste1, y_teste2, all_words, all_tags)\n",
    "\n",
    "\n",
    "with open('encoding/X_teste_NER.pkl', 'wb') as f:\n",
    "    pickle.dump(X_teste, f)\n",
    "with open('encoding/y_teste_NER.pkl', 'wb') as f:\n",
    "    pickle.dump(y_teste, f)\n",
    "with open('encoding/all_words_NER.pkl', 'wb') as f:\n",
    "    pickle.dump(all_words, f)\n",
    "with open('encoding/all_tags_NER.pkl', 'wb') as f:\n",
    "    pickle.dump(all_tags, f)\n",
    "with open('encoding/MAX_LEN_NER.pkl', 'wb') as f:\n",
    "    pickle.dump(MAX_LEN, f)\n",
    "\n",
    "model=simple_model(NUM_WORDS,MAX_LEN,NUM_TAGS)\n",
    "filename = 'checkpoints/model_NER.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=5,  \n",
    "    validation_data = [X_valid, y_valid],  \n",
    "    callbacks=[checkpoint],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
